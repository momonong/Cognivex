import torch
import torch.nn as nn
import torch.nn.functional as F
import nibabel as nib
import numpy as np
import os
from nilearn import plotting, datasets
from nilearn.image import resample_to_img

# ===============================================================
#  ç¬¬ä¸€éƒ¨åˆ†ï¼šè²¼ä¸Šæ‚¨å®Œæ•´çš„æ¨¡å‹å®šç¾©
# ===============================================================
class Squash(nn.Module):
    def forward(self, s):
        norm = torch.norm(s, dim=-1, keepdim=True)
        return (norm**2 / (1 + norm**2)) * (s / (norm + 1e-8))

class CapsuleLayer3D(nn.Module):
    def __init__(self, in_caps, out_caps, in_dim, out_dim, kernel_size=3, stride=2, padding=1):
        super().__init__()
        self.out_caps = out_caps
        self.out_dim = out_dim
        self.conv = nn.Conv3d(in_caps * in_dim, out_caps * out_dim, kernel_size, stride, padding)
        self.squash = Squash()
    def forward(self, x):
        B, in_caps, in_dim, D, H, W = x.size()
        x = x.view(B, in_caps * in_dim, D, H, W)
        x = self.conv(x)
        d, h, w = x.shape[2:]
        x = x.view(B, self.out_caps, self.out_dim, d, h, w)
        x = self.squash(x)
        return x

class CapsNet3D(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv3d(1, 16, kernel_size=5, stride=1, padding=0)
        self.conv2 = nn.Conv3d(16, 32, kernel_size=5, stride=2, padding=0)
        self.conv3 = nn.Conv3d(32, 64, kernel_size=5, stride=2, padding=0)
        self.caps1 = CapsuleLayer3D(1, 32, 64, 8)
        self.caps2 = CapsuleLayer3D(32, 32, 8, 8)
        self.final_caps = nn.Linear(32 * 8, 2 * 16)
        self.squash = Squash()
    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = x.unsqueeze(1)
        x = self.caps1(x)
        x = self.caps2(x)
        x = x.mean(dim=[3, 4, 5])
        x = x.view(x.size(0), -1)
        x = self.final_caps(x)
        x = x.view(x.size(0), 2, 16)
        x = self.squash(x)
        return x

class CapsNetRNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.capsnet = CapsNet3D()
        self.rnn = nn.RNN(input_size=32, hidden_size=64, batch_first=True)
        self.fc = nn.Linear(64, 1)
        self.activations = {}
        self._register_hooks()
    def _register_hooks(self):
        def hook_fn(module, input, output):
            self.activations['conv3'] = output.detach()
        self.capsnet.conv3.register_forward_hook(hook_fn)
    def forward(self, x):
        if x.dim() == 5: x = x.unsqueeze(2)
        B, C, T, D, H, W = x.size()
        feats = []
        for t in range(T):
            x_t = x[:, :, t, :, :, :]
            caps_out = self.capsnet(x_t)
            feats.append(caps_out.view(B, -1))
        feats = torch.stack(feats, dim=1)
        rnn_out, _ = self.rnn(feats)
        out = self.fc(rnn_out[:, -1, :])
        return torch.sigmoid(out).squeeze(1)

# ===============================================================
#  ç¬¬äºŒéƒ¨åˆ†ï¼šè²¼ä¸Šæˆ‘å€‘é©—è­‰éçš„è¼”åŠ©å‡½å¼
# ===============================================================
def select_strongest_channel(act: torch.Tensor, norm_type: str = "l2") -> int:
    if act.ndim == 3: return 0
    if norm_type == "l2":
        norms = torch.norm(act, p=2, dim=(1, 2, 3))
    elif norm_type == "l1":
        norms = torch.norm(act, p=1, dim=(1, 2, 3))
    else:
        raise ValueError(f"Unsupported norm_type: {norm_type}")
    print(f"-> åœ¨ {act.shape[0]} å€‹é »é“ä¸­ï¼Œç¬¬ {norms.argmax().item()} å€‹é »é“çš„ L2 Norm æœ€å¼·ã€‚")
    return norms.argmax().item()

def activation_to_nifti_nilearn(
    activation_tensor: torch.Tensor,
    activation_affine: np.ndarray,
    reference_nii_path: str,
    output_path: str,
):
    source_nii = nib.Nifti1Image(activation_tensor.cpu().numpy(), activation_affine)
    reference_nii = nib.load(reference_nii_path)
    resampled_nii = resample_to_img(
        source_img=source_nii,
        target_img=reference_nii,
        interpolation='continuous'
    )
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    nib.save(resampled_nii, output_path)

# ===============================================================
#  ç¬¬ä¸‰éƒ¨åˆ†ï¼šä¸»åŸ·è¡Œæµç¨‹
# ===============================================================
if __name__ == '__main__':
    print("--- é–‹å§‹åŸ·è¡Œæ¨¡å‹æ¿€æ´»åœ–å¯è¦–åŒ– ---")
    
    # --- 1. æº–å‚™å·¥ä½œ ---
    OUTPUT_DIR = "figures/final_visualization"
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    print("-> æ­£åœ¨ä¸‹è¼‰ MNI æ¨™æº–å¤§è…¦æ¨¡æ¿...")
    mni_template = datasets.load_mni152_template(resolution=2)
    REFERENCE_NII_PATH = os.path.join(OUTPUT_DIR, "mni_template.nii.gz")
    nib.save(mni_template, REFERENCE_NII_PATH)

    # --- 2. è¼‰å…¥æ¨¡å‹ä¸¦æ•æ‰æ¿€æ´»åœ– ---
    print("-> æ­£åœ¨å¯¦ä¾‹åŒ–æ¨¡å‹...")
    model = CapsNetRNN()
    model.eval()

    # å»ºç«‹ä¸€å€‹ç¬¦åˆæ¨¡å‹è¼¸å…¥çš„å‡ fMRI æ•¸æ“š
    # å°ºå¯¸: [Batch, Channels, Time, Depth, Height, Width]
    # æˆ‘å€‘ä½¿ç”¨ä¸€å€‹å¸¸è¦‹çš„ fMRI å°ºå¯¸ä½œç‚ºç¯„ä¾‹
    dummy_input = torch.randn(1, 1, 1, 91, 109, 91)
    
    print(f"-> æ­£åœ¨ç”¨å‡æ•¸æ“š ({dummy_input.shape}) é€²è¡Œä¸€æ¬¡å‰å‘å‚³æ’­...")
    with torch.no_grad():
        _ = model(dummy_input)
    
    # å¾ hook ä¸­å–å‡ºæˆ‘å€‘æ•æ‰åˆ°çš„æ¿€æ´»åœ–
    full_activation = model.activations['conv3']
    print(f"-> æˆåŠŸæ•æ‰åˆ° 'conv3' å±¤çš„æ¿€æ´»åœ–ï¼ŒåŸå§‹å½¢ç‹€: {full_activation.shape}")

    # --- 3. è™•ç†æ¿€æ´»åœ– ---
    # å»æ‰ Batch ç¶­åº¦ï¼Œå¾—åˆ° [C, D, H, W]
    activation_channels = full_activation[0]
    
    # é¸æ“‡æœ€å¼·çš„é‚£å€‹é »é“ï¼Œå¾—åˆ° [D, H, W]
    channel_idx = select_strongest_channel(activation_channels)
    activation_map = activation_channels[channel_idx]
    print(f"-> å·²é¸å–æœ€å¼·é »é“ï¼Œæœ€çµ‚æ¿€æ´»åœ–å½¢ç‹€: {activation_map.shape}")

    # --- 4. è¨ˆç®—ä½è§£æåº¦æ¿€æ´»åœ–çš„ Affine ---
    print("-> æ­£åœ¨ç‚ºä½è§£æåº¦æ¿€æ´»åœ–è¨ˆç®—æ­£ç¢ºçš„ Affine...")
    high_res_affine = mni_template.affine
    high_res_shape = mni_template.shape
    low_res_shape = activation_map.shape
    
    # è¨ˆç®—æ¯å€‹ç¶­åº¦çš„ç¸®æ”¾æ¯”ä¾‹
    scaling_factors = np.array(high_res_shape) / np.array(low_res_shape)
    
    # å»ºç«‹æ–°çš„ affine
    activation_affine = high_res_affine.copy()
    for i in range(3):
        activation_affine[i, i] *= scaling_factors[i]
    
    # --- 5. åŸ·è¡Œåº§æ¨™è½‰æ› ---
    print("-> æ­£åœ¨åŸ·è¡Œæˆ‘å€‘é©—è­‰éçš„ nilearn è½‰æ›å‡½å¼...")
    FINAL_NII_PATH = os.path.join(OUTPUT_DIR, "model_activation_map.nii.gz")
    activation_to_nifti_nilearn(
        activation_tensor=activation_map,
        activation_affine=activation_affine,
        reference_nii_path=REFERENCE_NII_PATH,
        output_path=FINAL_NII_PATH
    )

    # --- 6. ç”¢ç”Ÿæœ€çµ‚çš„ PNG ç–Šåœ– ---
    print("-> æ­£åœ¨ç”¢ç”Ÿæœ€çµ‚çš„ PNG é è¦½åœ–...")
    
    # ç‚ºäº†è®“åœ–åƒæ›´æ¸…æ™°ï¼Œæˆ‘å€‘æœƒéæ¿¾æ‰ 95% ä»¥ä¸‹çš„ä½ä¿¡è™Ÿ
    display = plotting.plot_stat_map(
        FINAL_NII_PATH,
        bg_img=REFERENCE_NII_PATH,
        title="Model Activation Map (conv3, strongest channel)",
        display_mode='ortho',
        colorbar=True,
        threshold=np.percentile(activation_map.numpy(), 95) # å‹•æ…‹è¨ˆç®—é–¾å€¼
    )
    
    FINAL_PNG_PATH = os.path.join(OUTPUT_DIR, "MODEL_ACTIVATION_PREVIEW.png")
    display.savefig(FINAL_PNG_PATH, dpi=300)

    print("\n" + "="*50)
    print("ğŸ‰ å¯è¦–åŒ–æµç¨‹å…¨éƒ¨å®Œæˆï¼")
    print(f"æœ€çµ‚çš„ NIfTI æª”æ¡ˆå„²å­˜æ–¼: {FINAL_NII_PATH}")
    print(f"æœ€çµ‚çš„ PNG é è¦½åœ–å„²å­˜æ–¼: {FINAL_PNG_PATH}")
    print("="*50)