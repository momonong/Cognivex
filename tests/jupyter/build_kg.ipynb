{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'output/activation_conv2_AD_AD_027_S_6648_task-rest_bold.nii_z001_t003.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 123\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    122\u001b[39m     act_path = \u001b[33m\"\u001b[39m\u001b[33moutput/activation_conv2_AD_AD_027_S_6648_task-rest_bold.nii_z001_t003.npy\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m     act = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mact_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m     act = np.squeeze(act)\n\u001b[32m    125\u001b[39m     G = build_semantic_graph()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/semantic-KG/.venv/lib/python3.11/site-packages/numpy/lib/_npyio_impl.py:451\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[39m\n\u001b[32m    449\u001b[39m     own_fid = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m     fid = stack.enter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    452\u001b[39m     own_fid = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    454\u001b[39m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'output/activation_conv2_AD_AD_027_S_6648_task-rest_bold.nii_z001_t003.npy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nilearn import datasets\n",
    "import nibabel as nib\n",
    "from skimage.transform import resize\n",
    "from collections import defaultdict\n",
    "# from utils.semantic_utils import generate_explanation\n",
    "import networkx as nx\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "def run_activation_to_top_regions(act_map, z_slice=50, top_k=5):\n",
    "    atlas = datasets.fetch_atlas_aal(version='SPM12')\n",
    "    aal_img = nib.load(atlas['maps'])\n",
    "    aal_data = aal_img.get_fdata()\n",
    "    aal_labels = atlas['labels']\n",
    "    aal_indices = atlas['indices']\n",
    "    label_dict = {int(k): v for k, v in zip(aal_indices, aal_labels)}\n",
    "\n",
    "    aal_slice = aal_data[:, :, z_slice]\n",
    "    resized_aal = resize(aal_slice, act_map.shape, order=0, preserve_range=True, anti_aliasing=False)\n",
    "    resized_aal = np.rint(resized_aal).astype(int)\n",
    "\n",
    "    region_activation = defaultdict(list)\n",
    "    for i in range(act_map.shape[0]):\n",
    "        for j in range(act_map.shape[1]):\n",
    "            label_id = resized_aal[i, j]\n",
    "            if label_id > 0:\n",
    "                region_activation[label_id].append(act_map[i, j])\n",
    "\n",
    "    region_mean = {k: np.mean(v) for k, v in region_activation.items()}\n",
    "    top = sorted(region_mean.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    return [(label_dict.get(k, f\"ID:{k}\"), v) for k, v in top]\n",
    "\n",
    "def build_semantic_graph():\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    region_to_network = {\n",
    "        \"Cuneus_L\": \"Visual-Dorsal Attention Network\",\n",
    "        \"Occipital_Sup_R\": \"Visual-Dorsal Attention Network\",\n",
    "        \"Occipital_Sup_L\": \"Visual-Dorsal Attention Network\",\n",
    "        \"Precuneus_L\": \"Posterior Cingulate-Precuneus\",\n",
    "        \"Precuneus_R\": \"Posterior Cingulate-Precuneus\",\n",
    "        \"Angular_L\": \"Language Network\",\n",
    "        \"Angular_R\": \"Language Network\",\n",
    "        \"Hippocampus_L\": \"Hippocampus-Amygdala\",\n",
    "        \"Amygdala_R\": \"Hippocampus-Amygdala\",\n",
    "        \"SupraMarginal_L\": \"Language Network\",\n",
    "        \"Postcentral_L\": \"Frontoparietal Network\",\n",
    "        \"Precentral_L\": \"Frontoparietal Network\",\n",
    "    }\n",
    "\n",
    "    for region, net in region_to_network.items():\n",
    "        G.add_node(region, **{\"type\": \"Region\"})\n",
    "        G.add_node(net, **{\"type\": \"Region\"})\n",
    "        G.add_edge(region, net, relation=\"part_of\")\n",
    "\n",
    "    nodes = [\n",
    "        (\"Memory & Emotion Coupling\", {\"type\": \"Function\"}),\n",
    "        (\"Self-referential Processing\", {\"type\": \"Function\"}),\n",
    "        (\"Visuospatial Processing\", {\"type\": \"Function\"}),\n",
    "        (\"Working Memory & Executive Control\", {\"type\": \"Function\"}),\n",
    "        (\"Stimulus Filtering & Attention Switching\", {\"type\": \"Function\"}),\n",
    "        (\"Language Comprehension & Expression\", {\"type\": \"Function\"}),\n",
    "        (\"Motor Planning\", {\"type\": \"Function\"}),\n",
    "        (\"Recent Memory Loss & Apathy\", {\"type\": \"Symptom\"}),\n",
    "        (\"Forgetfulness & Subjective Complaints\", {\"type\": \"Symptom\"}),\n",
    "        (\"Spatial Disorientation & Recognition Deficit\", {\"type\": \"Symptom\"}),\n",
    "        (\"Poor Planning & Problem Solving\", {\"type\": \"Symptom\"}),\n",
    "        (\"Irritability & Attention Instability\", {\"type\": \"Symptom\"}),\n",
    "        (\"Progressive Aphasia\", {\"type\": \"Symptom\"}),\n",
    "        (\"Fine Motor Impairment\", {\"type\": \"Symptom\"}),\n",
    "        (\"MCI\", {\"type\": \"Disease\"}),\n",
    "        (\"AD\", {\"type\": \"Disease\"}),\n",
    "        (\"Posterior Cortical Atrophy\", {\"type\": \"Disease\"}),\n",
    "        (\"Language Variant AD\", {\"type\": \"Disease\"}),\n",
    "    ]\n",
    "\n",
    "    edges = [\n",
    "        (\"Hippocampus-Amygdala\", \"Memory & Emotion Coupling\", {\"relation\": \"supports\"}),\n",
    "        (\"Memory & Emotion Coupling\", \"Recent Memory Loss & Apathy\", {\"relation\": \"manifests_as\"}),\n",
    "        (\"Recent Memory Loss & Apathy\", \"AD\", {\"relation\": \"associated_with\"}),\n",
    "\n",
    "        (\"Posterior Cingulate-Precuneus\", \"Self-referential Processing\", {\"relation\": \"supports\"}),\n",
    "        (\"Self-referential Processing\", \"Forgetfulness & Subjective Complaints\", {\"relation\": \"manifests_as\"}),\n",
    "        (\"Forgetfulness & Subjective Complaints\", \"MCI\", {\"relation\": \"associated_with\"}),\n",
    "\n",
    "        (\"Visual-Dorsal Attention Network\", \"Visuospatial Processing\", {\"relation\": \"supports\"}),\n",
    "        (\"Visuospatial Processing\", \"Spatial Disorientation & Recognition Deficit\", {\"relation\": \"manifests_as\"}),\n",
    "        (\"Spatial Disorientation & Recognition Deficit\", \"Posterior Cortical Atrophy\", {\"relation\": \"associated_with\"}),\n",
    "\n",
    "        (\"Frontoparietal Network\", \"Working Memory & Executive Control\", {\"relation\": \"supports\"}),\n",
    "        (\"Working Memory & Executive Control\", \"Poor Planning & Problem Solving\", {\"relation\": \"manifests_as\"}),\n",
    "        (\"Poor Planning & Problem Solving\", \"MCI\", {\"relation\": \"associated_with\"}),\n",
    "        (\"Poor Planning & Problem Solving\", \"AD\", {\"relation\": \"associated_with\"}),\n",
    "\n",
    "        (\"Salience Network\", \"Stimulus Filtering & Attention Switching\", {\"relation\": \"supports\"}),\n",
    "        (\"Stimulus Filtering & Attention Switching\", \"Irritability & Attention Instability\", {\"relation\": \"manifests_as\"}),\n",
    "        (\"Irritability & Attention Instability\", \"AD\", {\"relation\": \"associated_with\"}),\n",
    "\n",
    "        (\"Language Network\", \"Language Comprehension & Expression\", {\"relation\": \"supports\"}),\n",
    "        (\"Language Comprehension & Expression\", \"Progressive Aphasia\", {\"relation\": \"manifests_as\"}),\n",
    "        (\"Progressive Aphasia\", \"Language Variant AD\", {\"relation\": \"associated_with\"}),\n",
    "\n",
    "        (\"Frontoparietal Network\", \"Motor Planning\", {\"relation\": \"supports\"}),\n",
    "        (\"Motor Planning\", \"Fine Motor Impairment\", {\"relation\": \"manifests_as\"}),\n",
    "        (\"Fine Motor Impairment\", \"MCI\", {\"relation\": \"associated_with\"}),\n",
    "    ]\n",
    "\n",
    "    G.add_nodes_from(nodes)\n",
    "    G.add_edges_from([(u, v, d) for u, v, d in edges])\n",
    "    return G\n",
    "\n",
    "def analyze_fc1_neuron(model, neuron_index, conv_activation):\n",
    "    fc1_weights = model.fc1.weight.detach().cpu().numpy()\n",
    "    w = fc1_weights[neuron_index]\n",
    "    fmap = np.reshape(w, conv_activation.shape)\n",
    "    heatmap = fmap.mean(axis=0)\n",
    "    return heatmap\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    act_path = \"output/activation_conv2_AD_AD_027_S_6648_task-rest_bold.nii_z001_t003.npy\"\n",
    "    act = np.load(act_path)\n",
    "    act = np.squeeze(act)\n",
    "    G = build_semantic_graph()\n",
    "\n",
    "    import torch\n",
    "    from scripts.model import MCADNNet\n",
    "    model = MCADNNet(num_classes=2)\n",
    "    model.load_state_dict(torch.load(\"model/mcadnnet_mps.pth\", map_location=\"cpu\"))\n",
    "    model.eval()\n",
    "\n",
    "    neuron_id = 382\n",
    "    heatmap = analyze_fc1_neuron(model, neuron_id, act)\n",
    "    regions = run_activation_to_top_regions(heatmap, z_slice=50, top_k=3)\n",
    "\n",
    "    for region, score in regions:\n",
    "        print(f\"\\n🧠 fc1 neuron {neuron_id} 強活化區域：{region} ({score:.4f})\")\n",
    "        if region in G:\n",
    "            print(generate_explanation(region, G))\n",
    "        else:\n",
    "            print(\"⚠️ 無語意對應\")\n",
    "\n",
    "    # Optional: visualize graph structure with legend\n",
    "    type_colors = {\n",
    "        \"Region\": \"skyblue\",\n",
    "        \"Function\": \"lightgreen\",\n",
    "        \"Symptom\": \"orange\",\n",
    "        \"Disease\": \"tomato\"\n",
    "    }\n",
    "    node_colors = [type_colors.get(G.nodes[n].get(\"type\", \"Region\"), \"gray\") for n in G.nodes]\n",
    "\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    pos = nx.spring_layout(G, seed=42, k=0.6)\n",
    "    nx.draw(G, pos, with_labels=True, node_color=node_colors, node_size=2500, font_size=8, edge_color='gray')\n",
    "    edge_labels = nx.get_edge_attributes(G, \"relation\")\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=7)\n",
    "    legend_elements = [Patch(facecolor=c, label=t) for t, c in type_colors.items()]\n",
    "    plt.legend(handles=legend_elements, loc='upper left')\n",
    "    plt.title(\"Alzheimer's Disease Semantic Knowledge Graph (Prototype v1)\", fontsize=14)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../graphql/semantic_graph.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    nx.write_graphml(G, \"../graphql/alzheimers_kg_v1.graphml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[get_dataset_dir] Dataset found in /Users/morris/nilearn_data/aal_SPM12\n",
      "🧠 高活化腦區：\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nilearn import datasets\n",
    "import nibabel as nib\n",
    "from skimage.transform import resize\n",
    "from collections import defaultdict\n",
    "\n",
    "# 載入 activation map\n",
    "act = np.load(\"../../hook/activation_conv2.npy\")  # shape [50, H, W]\n",
    "act_map = act.mean(axis=0)  # 聚合 channel，shape [H, W]\n",
    "\n",
    "# 載入 AAL atlas\n",
    "atlas = datasets.fetch_atlas_aal(version='SPM12')\n",
    "aal_img = nib.load(atlas['maps'])\n",
    "aal_data = aal_img.get_fdata()\n",
    "aal_labels = atlas['labels']\n",
    "\n",
    "z_slice = 50  # 假設這是你 activation 所對應的切片\n",
    "aal_slice = aal_data[:, :, z_slice]  # shape [91, 109]\n",
    "\n",
    "# resize 到 activation 大小\n",
    "# 原始版本：\n",
    "# resized_aal = resize(aal_slice, act_map.shape, order=0, preserve_range=True, anti_aliasing=False).astype(int)\n",
    "\n",
    "# 改進版：保證輸出是純整數且 shape 正確\n",
    "resized_aal = resize(\n",
    "    aal_slice,\n",
    "    act_map.shape,\n",
    "    order=0,\n",
    "    preserve_range=True,\n",
    "    anti_aliasing=False\n",
    ")\n",
    "\n",
    "# 把小數強制轉為最近整數，再轉為 int（避免 label = 2.00001 這種浮點誤差）\n",
    "resized_aal = np.rint(resized_aal).astype(int)\n",
    "\n",
    "region_activation = defaultdict(list)\n",
    "\n",
    "for i in range(act_map.shape[0]):\n",
    "    for j in range(act_map.shape[1]):\n",
    "        value = resized_aal[i, j]\n",
    "\n",
    "        # 如果是 numpy 陣列，確認是大小為 1 才轉 scalar\n",
    "        if isinstance(value, np.ndarray):\n",
    "            if value.size == 1:\n",
    "                label_id = int(value.flat[0])\n",
    "            else:\n",
    "                continue  # 多維陣列就跳過\n",
    "        else:\n",
    "            try:\n",
    "                label_id = int(value)\n",
    "            except (ValueError, TypeError):\n",
    "                continue  # 無法轉換就跳過\n",
    "\n",
    "        # 篩掉背景（label_id <= 0）\n",
    "        if label_id > 0:\n",
    "            region_activation[label_id].append(act_map[i, j])\n",
    "\n",
    "\n",
    "# 平均每個腦區 activation\n",
    "region_mean_activation = {label_id: np.mean(vals) for label_id, vals in region_activation.items()}\n",
    "\n",
    "# 排名前幾高的腦區\n",
    "top_regions = sorted(region_mean_activation.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "top_named_regions = [(aal_labels[rid - 1], score) for rid, score in top_regions]\n",
    "\n",
    "print(\"🧠 高活化腦區：\")\n",
    "for name, score in top_named_regions:\n",
    "    print(f\"{name}: {score:.4f}\")\n",
    "\n",
    "kg_regions = [n for n, d in G.nodes(data=True) if d['type'] == 'Region']\n",
    "\n",
    "# 嘗試對應名稱（你可以做簡化比對，例如 substring matching）\n",
    "for name, score in top_named_regions:\n",
    "    for kg_node in kg_regions:\n",
    "        if name.lower() in kg_node.lower() or kg_node.lower() in name.lower():\n",
    "            print(f\"✅ 對應 KG node: {kg_node} (activation: {score:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (50, 9, 9) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m plt.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m4\u001b[39m))\n\u001b[32m      4\u001b[39m plt.subplot(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mact_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhot\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 這裡是 2D 的 activation map\u001b[39;00m\n\u001b[32m      6\u001b[39m plt.title(\u001b[33m\"\u001b[39m\u001b[33mActivation Map\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m plt.axis(\u001b[33m'\u001b[39m\u001b[33moff\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/semantic-KG/.venv/lib/python3.11/site-packages/matplotlib/pyplot.py:3601\u001b[39m, in \u001b[36mimshow\u001b[39m\u001b[34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, colorizer, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[39m\n\u001b[32m   3579\u001b[39m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes.imshow)\n\u001b[32m   3580\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mimshow\u001b[39m(\n\u001b[32m   3581\u001b[39m     X: ArrayLike | PIL.Image.Image,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3599\u001b[39m     **kwargs,\n\u001b[32m   3600\u001b[39m ) -> AxesImage:\n\u001b[32m-> \u001b[39m\u001b[32m3601\u001b[39m     __ret = \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3602\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3603\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3604\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3605\u001b[39m \u001b[43m        \u001b[49m\u001b[43maspect\u001b[49m\u001b[43m=\u001b[49m\u001b[43maspect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3606\u001b[39m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3607\u001b[39m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3608\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3609\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3610\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolorizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolorizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3611\u001b[39m \u001b[43m        \u001b[49m\u001b[43morigin\u001b[49m\u001b[43m=\u001b[49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3612\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3613\u001b[39m \u001b[43m        \u001b[49m\u001b[43minterpolation_stage\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolation_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3614\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilternorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilternorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3615\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilterrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilterrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3616\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3617\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3618\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3619\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3620\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3621\u001b[39m     sci(__ret)\n\u001b[32m   3622\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/semantic-KG/.venv/lib/python3.11/site-packages/matplotlib/__init__.py:1521\u001b[39m, in \u001b[36m_preprocess_data.<locals>.inner\u001b[39m\u001b[34m(ax, data, *args, **kwargs)\u001b[39m\n\u001b[32m   1518\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m   1519\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(ax, *args, data=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m   1520\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1521\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1523\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1524\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1526\u001b[39m     bound = new_sig.bind(ax, *args, **kwargs)\n\u001b[32m   1527\u001b[39m     auto_label = (bound.arguments.get(label_namer)\n\u001b[32m   1528\u001b[39m                   \u001b[38;5;129;01mor\u001b[39;00m bound.kwargs.get(label_namer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/semantic-KG/.venv/lib/python3.11/site-packages/matplotlib/axes/_axes.py:5979\u001b[39m, in \u001b[36mAxes.imshow\u001b[39m\u001b[34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, colorizer, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[39m\n\u001b[32m   5976\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5977\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_aspect(aspect)\n\u001b[32m-> \u001b[39m\u001b[32m5979\u001b[39m \u001b[43mim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5980\u001b[39m im.set_alpha(alpha)\n\u001b[32m   5981\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m im.get_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5982\u001b[39m     \u001b[38;5;66;03m# image does not already have clipping set, clip to Axes patch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/semantic-KG/.venv/lib/python3.11/site-packages/matplotlib/image.py:685\u001b[39m, in \u001b[36m_ImageBase.set_data\u001b[39m\u001b[34m(self, A)\u001b[39m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL.Image.Image):\n\u001b[32m    684\u001b[39m     A = pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m \u001b[38;5;28mself\u001b[39m._A = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_normalize_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[38;5;28mself\u001b[39m._imcache = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    687\u001b[39m \u001b[38;5;28mself\u001b[39m.stale = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/semantic-KG/.venv/lib/python3.11/site-packages/matplotlib/image.py:653\u001b[39m, in \u001b[36m_ImageBase._normalize_image_array\u001b[39m\u001b[34m(A)\u001b[39m\n\u001b[32m    651\u001b[39m     A = A.squeeze(-\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.\u001b[39;00m\n\u001b[32m    652\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (A.ndim == \u001b[32m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m A.ndim == \u001b[32m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m A.shape[-\u001b[32m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m3\u001b[39m, \u001b[32m4\u001b[39m]):\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for image data\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m A.ndim == \u001b[32m3\u001b[39m:\n\u001b[32m    655\u001b[39m     \u001b[38;5;66;03m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[32m    656\u001b[39m     \u001b[38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[32m    657\u001b[39m     \u001b[38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[32m    658\u001b[39m     \u001b[38;5;66;03m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[32m    659\u001b[39m     high = \u001b[32m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np.issubdtype(A.dtype, np.integer) \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: Invalid shape (50, 9, 9) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(act_map, cmap='hot')  # 這裡是 2D 的 activation map\n",
    "plt.title(\"Activation Map\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(resized_aal, cmap='tab20')  # 這是經過 resize 的 AAL label map\n",
    "plt.title(\"Resized AAL Slice\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act shape: (1, 50, 9, 9)\n",
      "act_map shape: (50, 9, 9)\n"
     ]
    }
   ],
   "source": [
    "print(\"act shape:\", act.shape)         # 預期為 (50, 9, 9)\n",
    "print(\"act_map shape:\", act_map.shape) # 預期為 (9, 9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation map shape: (50, 9, 9)\n",
      "AAL slice shape: (91, 109)\n",
      "resized AAL unique labels: [   0 2001 2002 2101 2102 2201 2202 2301 2302 2311 2312 2601 2602 4001\n",
      " 4002 4021 4022 5011 5012 5101 5102 5201 5202 6001 6211 6212 6221 6222\n",
      " 6301 6302]\n",
      "activation map 非零比例： 1.0\n"
     ]
    }
   ],
   "source": [
    "# DEBUG: 檢查 label 是否合理\n",
    "print(\"activation map shape:\", act_map.shape)\n",
    "print(\"AAL slice shape:\", aal_slice.shape)\n",
    "print(\"resized AAL unique labels:\", np.unique(resized_aal))\n",
    "print(\"activation map 非零比例：\", np.count_nonzero(act_map) / act_map.size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act shape: (1, 50, 9, 9)\n",
      "✅ act_map shape: (50, 9, 9)\n",
      "📢 act_map type: <class 'numpy.ndarray'>\n",
      "📢 act_map ndim: 3\n",
      "📢 act_map shape: (50, 9, 9)\n",
      "📢 act_map dtype: float32\n",
      "❌ act_map 不是 2D，你傳進的是錯的東西！\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "act = np.load(\"../../hook/activation_conv2.npy\")  # (50, 9, 9)\n",
    "print(\"act shape:\", act.shape)\n",
    "\n",
    "act_map = act.mean(axis=0)\n",
    "print(\"✅ act_map shape:\", act_map.shape)\n",
    "\n",
    "# 防呆 check：明確指出你拿的是誰\n",
    "print(\"📢 act_map type:\", type(act_map))\n",
    "print(\"📢 act_map ndim:\", act_map.ndim)\n",
    "print(\"📢 act_map shape:\", act_map.shape)\n",
    "print(\"📢 act_map dtype:\", act_map.dtype)\n",
    "\n",
    "# 安全可視化：只有 ndim=2 才能畫\n",
    "if act_map.ndim == 2:\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.imshow(act_map, cmap='hot')\n",
    "    plt.title(\"✅ Activation Map\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"❌ act_map 不是 2D，你傳進的是錯的東西！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squeezed act shape: (50, 9, 9)\n",
      "act_map shape: (9, 9)\n",
      "[get_dataset_dir] Dataset found in /Users/morris/nilearn_data/aal_SPM12\n",
      "act_map min/max: -0.08329132 -0.08329132\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0YAAAGGCAYAAABfU5GFAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAGplJREFUeJzt3Qm0reX8B/D31m28pVIpmidSKleE5B8aaCIzodJgbJASiW5REmUoEmmJZEykcjVbTRZFZSgk3QxZzYrmYf/X911rn7XPvufce07utc/p9/mstZ179nn33u9+d97n/T7P73n2lE6n02kAAAAKW2DQOwAAADBoghEAAFCeYAQAAJQnGAEAAOUJRgAAQHmCEQAAUJ5gBAAAlCcYAQAA5QlGAABAeYIR88Wuu+7arL766gN57UMPPbSZMmXKQF4bACZT25XXzGs/0eWaJNcmXT/72c/a956f0CUYFXX88ce3J4TnP//5j/s5br755vZkevXVVzf/a/fdd1/72hPthJZjmtsee+wx4t8PPvjgoW1uv/32//n+ATBnJ5988tB5OrepU6c2K620UntR/Y9//KOp7rrrrmuPy6KLLtr861//muv2P/nJT9rtn/a0pzWPPfbYqKFl++23f1z789vf/rZ53ete16y22mrtPuWz2mqrrZrjjjvucT0ftQlGRZ166qntieiXv/xl8+c///lxB6PDDjtsxGB04oknNn/84x+b+RmM8tojBaOPfOQjzf33398MSk7MP/jBD5qHHnpotr99+9vfbv8OwMT2sY99rDnllFOaE044odlmm22ab37zm83mm2/ePPDAA/Pl9Qbddo1VjsOKK67Y/vu0004b8/XGP//5z+bCCy+cp/ty+eWXN8997nOba665ptlzzz2bL3zhC23H5AILLNB8/vOfn+Nj/+///q893vkJXYJRQTfeeGN7MvnMZz7TLL/88u1Ja15baKGFmkUWWaQZhPTuDTJ8vOIVr2juueeeZubMmcPuzzHPsd9uu+0Gtm8AjE3C0Fvf+tb2QvurX/1qc8ABBzQ33HBD8+Mf//gJ2XaNRafTab71rW81O+20U7PtttvO9frh3nvvbc4444zm/e9/fzN9+vR5fr1xxBFHNEsttVRzxRVXtMEyn1U6Tc8555y2zZ2ThKcc7/yELv81FJQT0zLLLNNeoGf4ebQTVYbI99tvv7anJyFn5ZVXbnbeeee2BCwjNc973vPa7d7+9rcPlRykBKF/jtHDDz/cPPnJT26365cAkRNTGpzIKMshhxzSbLzxxu3Jbtq0ac2LX/zi5qKLLhp6zKxZs9pAFzkBdl+7WyM9Up32I4880nz84x9v1lprrfa9ZN8+/OEPNw8++OCIw/mXXnpps8kmm7T7tuaaazbf+MY3xnx8M4yfHqg0Hv3HfYMNNmie9axnzfaYSy65pHn961/frLrqqu3+rbLKKu2x7+89zHFdYoklmr/85S/Ny1/+8vb4pDwhPZtpsACYP9IWRcJRrz/84Q9tW5p2Lm1GRjD6w1PawbRX66yzTrvNsssu22y22WbNeeedN7RNf9uV831vSV/vrXdOUNqxGTNmNGuvvfZQ+3HggQfO1r7l97QraT+XXHLJ5pWvfGXz97//fVzH4LLLLmvb4De96U3t7eKLL57jc/zwhz9s27G0b9n+9NNPn6cjbvks1l9//WbppZee7W9PecpT5vjY0eYY/eIXv2hDX66T0sZuuOGGs40+jeUzZ3ISjArKBfprXvOaZuGFF27e/OY3N9dff33b29LrP//5T9sIpEZ36623bk8K73rXu9qTQU6Cz3zmM9uL8XjHO97RlhvkNtKQdEaPXv3qVzc/+tGPZisvy305WeeE2Q1K6Zl7yUte0hx11FHtyf+2225rQ0C3ZC8n9S996Uvtv/O83dfOexpNepESuJ7znOc0n/3sZ9tyiCOPPHLodXultDAnvNQoH3PMMe3JMQ3U73//+zEf4/SmnXnmme1x7Aaz73//++39I8nfUh747ne/uz3meb/5mSDa79FHH21HpVZYYYXmU5/6VBsi0yjmBsD8kUAQaRO60i684AUvaOfdfOhDH2rbjFxM77jjjm0o6EpblmD00pe+tC33ynzTdIT9+te/HvX13vnOdw61b93bW97ylmEX/Zmzk4Bz9NFHNzvssEPbbuS108698Y1vnK0d/NznPte26Z/85Cfbtnm8FQy5fkgHYzpG83qLL754WyI+p+3znlN6l/b23//+d9s2ziuZV/SrX/2q+d3vfjdPni9BNdcx1157bbPvvvu2n2f2/6yzzhr3Z84k1aGUK6+8MsMKnfPOO6/9/bHHHuusvPLKnX333XfYdocccki73emnnz7bc+QxccUVV7TbfO1rX5ttm1122aWz2mqrDf1+zjnntNueeeaZw7bbdtttO2uuuebQ74888kjnwQcfHLbNXXfd1VlhhRU6u+2229B9t912W/t8M2bMmO21c1/vf9pXX311+/see+wxbLsDDjigvf/CCy8cui/7nPsuvvjioftuvfXWziKLLNLZf//9O3OTx773ve/t3HnnnZ2FF164c8opp7T3n3322Z0pU6Z0Zs2aNbR/eQ9d991332zPdeSRR7aPuemmm4Yd1zx27733HvZ5bLfddu3r9T4nAOOXNi3n2fPPP789p/7tb3/rnHbaaZ3ll1++bQvye9cWW2zR2WCDDToPPPDAsHPypptu2llnnXWG7ttoo43a8/Sc9Ldd/a6//vrOUkst1dlqq63atjLSxiywwAKdSy65ZNi2J5xwQvtcl1122bB28D3vec+w7XbaaadR29J+Dz30UGfZZZftHHzwwcMen/c2kltuuaUzderUzoknnjh0X47Lq171qtm2Tds7t+MzknPPPbez4IILtrcXvvCFnQMPPLC93si+jvQaaUO7Lrroova952fkmK6xxhrtdrnuGOm6ZzyfOZOTEaNi0nuTkYb0gESGkdOr9J3vfKcdiejK4gEbbbRROyLT7/EsJ/qyl72sWW655Zrvfve7Q/fdddddbe9Mb6/Wggsu2I5kdXvC7rzzzna0JcPUc+pZm9uKOJEa5177779/+/Pss88edv966603VDLRHaF6xjOe0ZavjVV6FDOq0+1JS1ndpptu2vZujWSxxRYbVpOdcsVsn6x11VVXzbb9XnvtNezzyO8ZjTv//PPHvI8AjG7LLbdsz/8pTUsVQUYFUi6VsvJI+5TFBN7whje0IyE5b+d2xx13tKP+qcbormKXUq+MNOS+xyPtQtrjtC1pV9JWdqsNUsGx7rrrDr1+bmlzo1uG3m0H99lnn2HP+773vW/M+5B5s3lvqTTpyr+z8MFIFRW5rsj8nde+9rXDts/zpP2fF1LZ8fOf/7wdNct+pIoixz4l7eMtbUtbm3nAOSb9pXnd657xfOZMToJRIQk+OVElFOX//CkZyy1Ldt9yyy3NBRdcMKxud6S5MP/NpNKcHDMJs1v3nFrj1F33D/d//etfb2t6u3XYaZgSXu6+++7H9do33XRTe3JO/XWvDO3n5Je/90p5Q780RuM9kadsLsHvr3/9a1syOFoZXWSblOulXjlziPKeU+4X/e877yXznno9/elPH1bqAcB/54tf/GJ7Ds/Ka5lzkgvg3kWF0n6m8+qjH/1oe87uvXVLm2+99db2Z0rPM2835+rMNf3ABz7Q/OY3vxnzvmTFtbTLKdVKu9iVC/GEkv7X77YJ3dfvtoMpg+uVTr/xrEa3xhprtMege/2Q50s53UhzlbN95uomNHS3zwIM6cRLoJtXUtaX64m00Vlp96CDDmpDS8JsSuLGqjt3bE7XPuP5zJmcpg56B/jfSS9HlstMOMqtX05sqT2eX1Jf/OUvf7ntLUot7ve+9722lysjU70n0gSE/D0NR+qo0zOW+UD9E17Ha6wjXd2euH7jXdwgPVhpQHbZZZc2DKaHabTAml6v9ER98IMfbI9JeibT65RjMdr3PgAw/+SiPtUKkTYpiyWkgytfRZEOrO65OYsHZbRgJN0OucxbSRuWzsFzzz23nUubeUBZCny0773ryhzfjBKlfXz2s5897G/ZhwStrDI7kox2zQuZ/5u5QVk4IQtI9EtVRFaI67azvXOXR9o+1xuZnzwvpdokISm3BMMs+JQANi/n347nM2dyEowKyYkoQSO9YP3S25KeqJykU9aVXqC5TWYcb0ldGoanPvWpbTldGpgEtUxA7ZWeuYyGZH96n7//xDae1075Wk5mOVGn5KAro2TpwRutvO2/leOYxjSNWZZ9TSnhaF9O96c//akdKetdbKF3taJeeS8p6+v2CEYeH92VAAGYd7oddN3FEzLpvjtyn0UMUnY3N93VWXPLwjxpE7Mow5yCUVYszUV4yru6Cy/0SludErIttthiju1itx1MOOsdJRrr9w12V5PLwkf9bVmeI0tlZ8W6tO3d640clywY0d/ZmFVfjz322LZSYqQKjXmhG2jTGTxW3dG0XPuM9nmO9zNn8lFKV0SWy8yJLUtRZ3i5/5Y5Khl67tbkpuwtJ9uRVljpjpxkVCPG8s3XkWH8vFZ6nXKyzNyh/jK67gm0d3QmS2emhrhXhu7H+topgYisxtOr28M2P79XKA1aQl2G3Ucz0nvOv+f05XRpmHu3ze85UadxBGDey2qpGUVKW5KQkI7G3JdKiJEuwLOialfKyXplxCkjC/1LavfKc6bSIGHj05/+9Ijb5O+pLsiXqo/U7mduUqRzLhJIevW3i6NJB19CQVan7b9+SDuX99NbTpd/Z65u2vj+7VMNEnNazW6sModqpGqO7pyq8ZQKZtXalArmmPRfW3RfYzyfOZOTEaMiEngSfFLeNZIsPdn9stecyHLiyuhNvntgt912a5eETqlXniejSil/S+9K5ujk93wnQoJS5ivlxDKaPHeWE01YyPB/7whOJLglwGWSaQJL5kLl+bMgQnfp6+5oTO7L6FNGTtITl7rgkWqDs68pZ/vKV77Snuwydyd1yBmhyYhOdyGK+SGv3VsqOJKUzuVYpnFJA/ekJz2pXfxitDlNmXv105/+tH1POd4pTcwcrHwvU/f7nQCY99I2pl3Md/YlJKQCI8El7VnmASU8pBohnXn5aot0MEbaq1xQpy1Ne3XllVe2bWzvQjr9slBCLrTznUT95e+Zh5vb2972trYsPfuSkPCiF72oLc/OV2vk/nzRaUZPUoKXhQ+OP/74dt5qFvfJvOLMmZmbm2++uX3u/oUbulIynrKylK0leGWhpDzvaO8tCyMkhOR6I+XjXXnM4YcfPtv2mZc0Wgfm3nvv3X7VRa4Z0pZm/lK+2DXXBqmgGOn7E+fUeZsRsSxDnuOVx6bKJccy87hyLGOsnzmT1KCXxeN/Y4cddugsuuiinXvvvXfUbXbdddfOQgst1Ln99tvb3++4447OXnvt1VlppZXapaCzrHeWuuz+Pc4444zOeuut1y7J2bt0d/9y3b1LWq6yyirttocffviIf//EJz7RPjbLok6fPr1z1llnjfh8l19+eWfjjTdu9613udGRljx9+OGHO4cddli7FGfeY/bhoIMOGrbc5pyWDN18883b21iX656TkZbrvvbaaztbbrllZ4kllugst9xynT333LNzzTXXzLYceo7DtGnTOjfccENn66237iy++OLtUuZ5zkcffXSu+wfA2JbrzldS9Mt5dq211mpv3SWzcz7eeeedOyuuuGLbvqTN3H777dslvrvS3m2yySadpZdeurPYYot11l133c4RRxwxbFnp/rYrbU5+H+nWu7x2nuOoo47qrL/++m27ucwyy7RtY9q8u+++e2i7+++/v7PPPvu0S26nHcl1QZYen9ty3cccc0y7zQUXXDDqNieffHK7Ta4J8nUS+XeOy2gOPfTQdpu0c71flTHSbffddx/1eWbOnNl+lUeOZ9rPXA+svfba7T5kufDxLNfddemll7ZLoi+55JLtcdpwww07xx133LBtxvKZMzlNyf8MOpwBY5PFGNLL2Dt6BgDAf88cIwAAoDzBCAAAKE8wAgAAyjPHCAAAKM+IEQAAUJ5gBAAAlCcYAQAA5U0d64bTpkyZv3sCQOteUz9LWP1DZw96FwBKmPXJ7ca0nREjAACgPMEIAAAoTzACAADKE4wAAIDyBCMAAKA8wQgAAChPMAIAAMoTjAAAgPIEIwAAoDzBCAAAKE8wAgAAyhOMAACA8gQjAACgPMEIAAAoTzACAADKE4wAAIDyBCMAAKA8wQgAAChPMAIAAMoTjAAAgPIEIwAAoDzBCAAAKE8wAgAAyhOMAACA8gQjAACgPMEIAAAoTzACAADKE4wAAIDyBCMAAKA8wQgAAChPMAIAAMoTjAAAgPIEIwAAoDzBCAAAKE8wAgAAyhOMAACA8gQjAACgPMEIAAAob+qgdwAAYLKbsfv0Qe/CpPOCHbZoJqpn7rdPM1GtfuOqg96FJywjRgAAQHmCEQAAUJ5gBAAAlCcYAQAA5QlGAABAeYIRAABQnmAEAACUJxgBAADlCUYAAEB5ghEAAFCeYAQAAJQnGAEAAOUJRgAAQHmCEQAAUJ5gBAAAlCcYAQAA5QlGAABAeYIRAABQnmAEAACUJxgBAADlCUYAAEB5ghEAAFCeYAQAAJQnGAEAAOUJRgAAQHmCEQAAUJ5gBAAAlCcYAQAA5QlGAABAeYIRAABQnmAEAACUJxgBAADlCUYAAEB5ghEAAFCeYAQAAJQnGAEAAOUJRgAAQHlTB70DAABjcdLW+zQT1e4nHTvoXZh0Zu03cT/PiWzmjw5oJqJtdjy6meyMGAEAAOUJRgAAQHmCEQAAUJ5gBAAAlCcYAQAA5QlGAABAeYIRAABQnmAEAACUJxgBAADlCUYAAEB5ghEAAFCeYAQAAJQnGAEAAOUJRgAAQHmCEQAAUJ5gBAAAlCcYAQAA5QlGAABAeYIRAABQnmAEAACUJxgBAADlCUYAAEB5ghEAAFCeYAQAAJQnGAEAAOUJRgAAQHmCEQAAUJ5gBAAAlCcYAQAA5QlGAABAeYIRAABQnmAEAACUJxgBAADlCUYAAEB5ghEAAFCeYAQAAJQnGAEAAOUJRgAAQHlTB70DAMDEctLW+zQT0bTFT20mrjsGvQOTzuo3rjroXZiUZg56B57AjBgBAADlCUYAAEB5ghEAAFCeYAQAAJQnGAEAAOUJRgAAQHmCEQAAUJ5gBAAAlCcYAQAA5QlGAABAeYIRAABQnmAEAACUJxgBAADlCUYAAEB5ghEAAFCeYAQAAJQnGAEAAOUJRgAAQHmCEQAAUJ5gBAAAlCcYAQAA5QlGAABAeYIRAABQnmAEAACUJxgBAADlCUYAAEB5ghEAAFCeYAQAAJQnGAEAAOUJRgAAQHmCEQAAUJ5gBAAAlCcYAQAA5QlGAABAeYIRAABQnmAEAACUJxgBAADlCUYAAEB5Uwe9AwDAxDJt8VObiei6ZVdtJqpLm4ebiWqz5p5mIrq0edKgd2FSOuMlX2wmpvubyc6IEQAAUJ5gBAAAlCcYAQAA5QlGAABAeYIRAABQnmAEAACUJxgBAADlCUYAAEB5ghEAAFCeYAQAAJQnGAEAAOUJRgAAQHmCEQAAUJ5gBAAAlCcYAQAA5QlGAABAeYIRAABQnmAEAACUJxgBAADlCUYAAEB5ghEAAFCeYAQAAJQnGAEAAOUJRgAAQHmCEQAAUJ5gBAAAlCcYAQAA5QlGAABAeYIRAABQnmAEAACUJxgBAADlCUYAAEB5ghEAAFCeYAQAAJQnGAEAAOUJRgAAQHmCEQAAUN7UQe8AADCx3HvfW5qJaNX7mglrs+bYQe/CpLNZc8+gd2FSmvmzA5qJ6NM7Ht1MdkaMAACA8gQjAACgPMEIAAAoTzACAADKE4wAAIDyBCMAAKA8wQgAAChPMAIAAMoTjAAAgPIEIwAAoDzBCAAAKE8wAgAAyhOMAACA8gQjAACgPMEIAAAoTzACAADKE4wAAIDyBCMAAKA8wQgAAChPMAIAAMoTjAAAgPIEIwAAoDzBCAAAKE8wAgAAyhOMAACA8gQjAACgPMEIAAAoTzACAADKE4wAAIDyBCMAAKA8wQgAAChPMAIAAMoTjAAAgPIEIwAAoDzBCAAAKE8wAgAAyhOMAACA8gQjAACgvKmD3gEAYGLZ/dxjm4loxu7Tm4nrqkHvwKQza42/NhPVdZ+dmP8fiG12PHrQu/CEZcQIAAAoTzACAADKE4wAAIDyBCMAAKA8wQgAAChPMAIAAMoTjAAAgPIEIwAAoDzBCAAAKE8wAgAAyhOMAACA8gQjAACgPMEIAAAoTzACAADKE4wAAIDyBCMAAKA8wQgAAChPMAIAAMoTjAAAgPIEIwAAoDzBCAAAKE8wAgAAyhOMAACA8gQjAACgPMEIAAAoTzACAADKE4wAAIDyBCMAAKA8wQgAAChPMAIAAMoTjAAAgPIEIwAAoDzBCAAAKE8wAgAAyhOMAACA8gQjAACgPMEIAAAoTzACAADKmzroHQAAGIvDTrqqmahm7D590Lsw6Vy3wwHNRLXNjkcPehcYACNGAABAeYIRAABQnmAEAACUJxgBAADlCUYAAEB5ghEAAFCeYAQAAJQnGAEAAOUJRgAAQHmCEQAAUJ5gBAAAlCcYAQAA5QlGAABAeYIRAABQnmAEAACUJxgBAADlCUYAAEB5ghEAAFCeYAQAAJQnGAEAAOUJRgAAQHmCEQAAUJ5gBAAAlCcYAQAA5QlGAABAeYIRAABQnmAEAACUJxgBAADlCUYAAEB5ghEAAFCeYAQAAJQnGAEAAOUJRgAAQHmCEQAAUJ5gBAAAlCcYAQAA5QlGAABAeVMHvQMAAJPdYSddNehdmHx2PHrQewDDGDECAADKE4wAAIDyBCMAAKA8wQgAAChPMAIAAMoTjAAAgPIEIwAAoDzBCAAAKE8wAgAAyhOMAACA8gQjAACgPMEIAAAoTzACAADKE4wAAIDyBCMAAKA8wQgAAChPMAIAAMoTjAAAgPIEIwAAoDzBCAAAKE8wAgAAyhOMAACA8gQjAACgPMEIAAAoTzACAADKE4wAAIDyBCMAAKA8wQgAAChPMAIAAMoTjAAAgPIEIwAAoDzBCAAAKE8wAgAAyhOMAACA8gQjAACgPMEIAAAoTzACAADKE4wAAIDypnQ6nc6gdwIAAGCQjBgBAADlCUYAAEB5ghEAAFCeYAQAAJQnGAEAAOUJRgAAQHmCEQAAUJ5gBAAAlCcYAQAATXX/Dw2Zfw0Gjr51AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn import datasets\n",
    "import nibabel as nib\n",
    "from skimage.transform import resize\n",
    "\n",
    "# === Step 1: Load activation ===\n",
    "act = np.load(\"../../hook/activation_conv2.npy\")  # shape: (1, 50, 9, 9)\n",
    "act = np.squeeze(act)  # shape: (50, 9, 9)\n",
    "print(\"Squeezed act shape:\", act.shape)\n",
    "\n",
    "act_map = act.mean(axis=0)  # shape: (9, 9)\n",
    "print(\"act_map shape:\", act_map.shape)\n",
    "\n",
    "# === Step 2: Load atlas and slice ===\n",
    "atlas = datasets.fetch_atlas_aal(version='SPM12')\n",
    "aal_img = nib.load(atlas['maps'])\n",
    "aal_data = aal_img.get_fdata()\n",
    "aal_labels = atlas['labels']\n",
    "\n",
    "z_slice = 50\n",
    "aal_slice = aal_data[:, :, z_slice]  # shape: (91, 109)\n",
    "resized_aal = resize(\n",
    "    aal_slice, act_map.shape, order=0, preserve_range=True, anti_aliasing=False\n",
    ")\n",
    "resized_aal = np.rint(resized_aal).astype(int)\n",
    "\n",
    "# === Step 3: Visual check ===\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(act_map, cmap='hot')\n",
    "print(\"act_map min/max:\", act_map.min(), act_map.max())\n",
    "plt.title(\"Activation Map\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(resized_aal, cmap='tab20')\n",
    "plt.title(\"Resized AAL Slice\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Top activated brain regions:\n",
      "SupraMarginal_R (6212): -0.0833\n",
      "Precentral_R (2002): -0.0833\n",
      "Frontal_Inf_Tri_R (2312): -0.0833\n",
      "Occipital_Mid_R (5202): -0.0833\n",
      "Frontal_Inf_Oper_R (2302): -0.0833\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# 重新計算 region activation\n",
    "region_activation = defaultdict(list)\n",
    "\n",
    "for i in range(act_map.shape[0]):\n",
    "    for j in range(act_map.shape[1]):\n",
    "        label_id = resized_aal[i, j]\n",
    "        if label_id > 0:\n",
    "            region_activation[label_id].append(act_map[i, j])\n",
    "\n",
    "# 計算平均\n",
    "region_mean = {k: np.mean(v) for k, v in region_activation.items()}\n",
    "top5 = sorted(region_mean.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "# 正確對應 label 名稱\n",
    "label_dict = {int(k): v for k, v in zip(atlas['indices'], atlas['labels'])}\n",
    "\n",
    "print(\"🧠 Top activated brain regions:\")\n",
    "for label_id, score in top5:\n",
    "    label_name = label_dict.get(label_id, \"Unknown\")\n",
    "    print(f\"{label_name} ({label_id}): {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_activation(start_node, G, max_depth=3):\n",
    "    visited = set()\n",
    "    result = []\n",
    "    queue = [(start_node, 0)]\n",
    "\n",
    "    while queue:\n",
    "        current, depth = queue.pop(0)\n",
    "        if depth >= max_depth:\n",
    "            continue\n",
    "\n",
    "        # 如果節點不是圖中的節點就跳過\n",
    "        if current not in G:\n",
    "            continue\n",
    "\n",
    "        for neighbor in G.successors(current):\n",
    "            relation = G.edges[current, neighbor]['relation']\n",
    "            result.append((current, neighbor, relation))\n",
    "            if neighbor not in visited:\n",
    "                queue.append((neighbor, depth + 1))\n",
    "                visited.add(neighbor)\n",
    "\n",
    "    return result\n",
    "\n",
    "def explain_naturally(chain):\n",
    "    explanation = []\n",
    "    for src, tgt, rel in chain:\n",
    "        if rel == \"supports\":\n",
    "            explanation.append(f\"{src} 支援了 {tgt} 的功能\")\n",
    "        elif rel == \"manifests_as\":\n",
    "            explanation.append(f\"{tgt} 是 {src} 功能受損後的表現\")\n",
    "        elif rel == \"associated_with\":\n",
    "            explanation.append(f\"{src} 通常與 {tgt} 有關\")\n",
    "        elif rel == \"part_of\":\n",
    "            explanation.append(f\"{src} 是 {tgt} 的一部分\")\n",
    "        else:\n",
    "            explanation.append(f\"{src} --{rel}→ {tgt}\")\n",
    "    return \"。\".join(explanation) + \"。\"\n",
    "\n",
    "def generate_explanation(region, G):\n",
    "    chain = explain_activation(region, G)\n",
    "    segments = {rel: [] for rel in [\"part_of\", \"supports\", \"manifests_as\", \"associated_with\"]}\n",
    "\n",
    "    for src, tgt, rel in chain:\n",
    "        segments[rel].append((src, tgt))\n",
    "\n",
    "    text = f\"模型顯著活化了 {region}，\"\n",
    "\n",
    "    if segments[\"part_of\"]:\n",
    "        _, net = segments[\"part_of\"][0]\n",
    "        text += f\"該區域屬於 {net} 功能網絡，\"\n",
    "\n",
    "    if segments[\"supports\"]:\n",
    "        _, func = segments[\"supports\"][0]\n",
    "        text += f\"與 {func} 相關。\"\n",
    "\n",
    "    if segments[\"manifests_as\"]:\n",
    "        symp = segments[\"manifests_as\"][0][1]\n",
    "        text += f\"該功能受損時常見表現為 {symp}，\"\n",
    "\n",
    "    if segments[\"associated_with\"]:\n",
    "        dis = segments[\"associated_with\"][0][1]\n",
    "        text += f\"這可能與 {dis} 有關。\"\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
