# Cognivex: Explainable AI Framework for fMRI-based Alzheimer's Disease Analysis

**Cognivex** is a multi-agent explainable artificial intelligence framework specifically designed for Alzheimer's Disease functional magnetic resonance imaging (fMRI) analysis. The system integrates deep learning models, knowledge graph reasoning, and large language models to provide a complete automated analysis pipeline from raw fMRI data to clinical reports.

## ğŸ¯ Core Mission

Solving the "black box" problem in neuroimaging AI by creating a trustworthy, autonomous AI assistant that transforms raw fMRI data into clinically relevant, explainable reports for neuroscientists.

## ğŸ—ï¸ Key Features

* **ğŸ§  Intelligent Multi-Agent System**: 7-node sequential processing pipeline based on LangGraph
* **ğŸ” Dynamic Explainable Layer Selection**: LLM-driven intelligent selection of the most meaningful neural network layers for visualization
* **ğŸ”— Knowledge Graph Integration**: Neo4j graph database combined with GraphRAG for semantic reasoning
* **ğŸ“„ Bilingual Report Generation**: Automatic generation of clinical analysis reports in both Chinese and English
* **ğŸ–¥ï¸ Interactive Web Interface**: User-friendly Streamlit-based interface
* **ğŸ”¬ Scientific Validation**: Automatic identification of Default Mode Network (DMN) activation patterns

## ğŸ”„ Technical Highlights

* **Model-Agnostic Design**: Supports multiple deep learning models (CapsNet-RNN, MCADNNet)
* **Coordinate System Correction**: Fixed dimensional mapping errors, improving detection from 1 to 54 brain regions
* **Complete State Management**: Smart UI locking system prevents misoperations during analysis
* **Real-time Progress Tracking**: Phase-by-phase progress display and status updates

---

## ğŸ”„ LangGraph Workflow Architecture

```mermaid
graph LR
    A[START] --> B[æ¨ç†ç¯€é»<br/>Inference Node]
    B --> C[ç¯©é¸ç¯€é»<br/>Filtering Node] 
    C --> D[å¾Œè™•ç†ç¯€é»<br/>Post-processing Node]
    D --> E[å¯¦é«”é€£çµç¯€é»<br/>Entity Linking Node]
    E --> F[çŸ¥è­˜æ¨ç†ç¯€é»<br/>Knowledge Reasoning Node]
    F --> G[å½±åƒè§£é‡‹ç¯€é»<br/>Image Explanation Node]
    G --> H[å ±å‘Šç”Ÿæˆç¯€é»<br/>Report Generation Node]
    H --> I[END]
```

## ğŸ“‹ System Requirements

### Hardware Requirements
- **GPU**: NVIDIA GPU with CUDA support (recommended for training/inference)
- **Memory**: 16GB+ RAM for fMRI data processing
- **Storage**: 50GB+ for datasets and model weights
- **Database**: Running Neo4j database instance (local or remote)

### Software Requirements
- **OS**: Ubuntu 20.04+ / macOS 12+ / Windows 11
- **Python**: 3.11+ (configured: `>=3.11,<3.14`)
- **CUDA**: CUDA 11.8+ (for GPU acceleration)
- **Docker**: Docker Desktop (optional, for Neo4j)

---

## ğŸš€ Installation

### Method 1: Poetry (Recommended)

```bash
# Clone repository
git clone <repository-url>
cd semantic-KG

# Install Poetry dependencies
poetry install

# Activate virtual environment
poetry shell

# Install PyTorch with CUDA support
poetry run poe autoinstall-torch-cuda
```

### Method 2: pip

```bash
# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install PyTorch with CUDA (if needed)
python -m pip install light-the-torch
python -m light_the_torch install --upgrade torch torchaudio torchvision
```

### Environment Configuration

Create `.env` file in root directory:

```bash
# Neo4j Knowledge Graph
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your_neo4j_password

# Primary LLM Provider: Google Vertex AI
GOOGLE_CLOUD_PROJECT=your_gcp_project_id
GOOGLE_CLOUD_LOCATION=us-central1
GOOGLE_APPLICATION_CREDENTIALS=./gcp-service-account.json
GOOGLE_GENAI_USE_VERTEXAI=1

# Backup LLM Provider: AWS Bedrock
AWS_ACCESS_KEY_ID=your_aws_access_key
AWS_SECRET_ACCESS_KEY=your_aws_secret_key
AWS_DEFAULT_REGION=us-east-1

# Local LLM Provider: Ollama
OLLAMA_BASE_URL=http://localhost:11434

# Optional: Cloud deployment
PROJECT_ID=your_gcp_project_id
LOCATION=your_gcp_location
BUCKET_ID=your_gcp_bucket
```

## ğŸ¤– LLM Provider Selection Guide

Cognivex supports multiple LLM providers. Choose based on your requirements:

### ğŸŒŸ Primary: Google Vertex AI Gemini (Recommended)

**Advantages:**
- Superior multimodal capabilities (text + image analysis)
- Optimized for clinical imaging tasks
- Enterprise-grade reliability and security

**Models Supported:**
- `gemini-1.5-flash`: Fast inference, cost-effective
- `gemini-1.5-pro`: Advanced reasoning, higher accuracy

**Setup:**
```bash
# Download GCP service account key (JSON file)
# Place in project root: gcp-service-account.json

# Verify connection
python -c "from app.services.llm_providers.gemini import handle_chat; print('Vertex AI ready')"
```

### ğŸ­ Backup: AWS Bedrock Claude

**Advantages:**
- Excellent text understanding and generation
- Cost-effective for text-only tasks
- Strong enterprise support

**Models Supported:**
- `anthropic.claude-haiku-4-5-20251001-v1:0`: Fast, economical
- Other Claude variants available

**Setup:**
```bash
# Configure AWS credentials
export AWS_ACCESS_KEY_ID="your_access_key"
export AWS_SECRET_ACCESS_KEY="your_secret_key"
export AWS_DEFAULT_REGION="us-east-1"

# Verify connection
python -c "from app.services.llm_providers.bedrock import handle_text; print('Bedrock ready')"
```

### ğŸ–¥ï¸ Local: Ollama (Development/Offline)

**Advantages:**
- Complete privacy and data security
- No internet dependency
- No per-use costs

**Models Supported:**
- `llama3.2`: General-purpose reasoning
- `qwen2.5:14b`: Advanced Chinese/English bilingual
- Custom fine-tuned models

**Setup:**
```bash
# Install and start Ollama
curl -fsSL https://ollama.ai/install.sh | sh
ollama serve

# Download models
ollama pull llama3.2
ollama pull qwen2.5:14b

# Verify connection
curl http://localhost:11434/api/tags
```

### âš™ï¸ Runtime Provider Selection

You can switch providers dynamically:

```python
from app.services.llm_providers import llm_response, llm_image_response

# Text-only analysis
result = llm_response(
    prompt="Analyze this clinical data...",
    llm_provider="gemini"  # or "aws_bedrock", "gpt-oss-20b"
)

# Multimodal analysis (text + images)
result = llm_image_response(
    prompt="Explain this fMRI activation map...",
    image_path="/path/to/brain_scan.png",
    llm_provider="gemini"  # Only Gemini and Bedrock support images
)
```

**Provider Comparison:**

| Provider | Text | Images | Cost | Privacy | Offline |
|----------|------|---------|------|---------|----------|
| **Gemini** | âœ… Excellent | âœ… Best | ğŸŸ¡ Medium | ğŸŸ¡ Cloud | âŒ No |
| **Bedrock** | âœ… Excellent | âœ… Good | âœ… Low | ğŸŸ¡ Cloud | âŒ No |
| **Ollama** | ğŸŸ¡ Good | âŒ No | âœ… Free | âœ… Complete | âœ… Yes |

---

## ğŸ“‚ Data Directory Structure

```
semantic-KG/
â”œâ”€â”€ app/                      # LangGraph Analysis Pipeline
â”‚   â”œâ”€â”€ agents/               # Pipeline node implementations
â”‚   â”‚   â”œâ”€â”€ inference.py      # Model inference and classification
â”‚   â”‚   â”œâ”€â”€ filtering.py      # Dynamic layer filtering
â”‚   â”‚   â”œâ”€â”€ postprocessing.py # Activation map processing
â”‚   â”‚   â”œâ”€â”€ entity_linking.py # Brain region entity linking
â”‚   â”‚   â”œâ”€â”€ knowledge_reasoning.py # Neo4j knowledge integration
â”‚   â”‚   â”œâ”€â”€ image_explainer.py # Visual analysis
â”‚   â”‚   â””â”€â”€ report_generator.py # Clinical report synthesis
â”‚   â”œâ”€â”€ core/                 # Core processing tools
â”‚   â”‚   â”œâ”€â”€ fmri_processing/  # fMRI analysis pipeline
â”‚   â”‚   â”œâ”€â”€ knowledge_graph/  # Knowledge graph query tools
â”‚   â”‚   â””â”€â”€ vision/           # Image explanation tools
â”‚   â”œâ”€â”€ graph/                # LangGraph workflow definition
â”‚   â”‚   â”œâ”€â”€ state.py          # AgentState schema
â”‚   â”‚   â””â”€â”€ workflow.py       # Complete pipeline workflow
â”‚   â””â”€â”€ services/             # External service connectors
â”‚       â”œâ”€â”€ llm_providers/    # Modular LLM provider system
â”‚       â”‚   â”œâ”€â”€ __init__.py   # Unified call interface  
â”‚       â”‚   â”œâ”€â”€ gemini.py     # Google Vertex AI Gemini
â”‚       â”‚   â”œâ”€â”€ bedrock.py    # AWS Bedrock Claude
â”‚       â”‚   â””â”€â”€ ollama.py     # Ollama local inference
â”‚       â””â”€â”€ neo4j_connector.py # Neo4j database interface
â”œâ”€â”€ data/                     # fMRI datasets (AD/CN subjects)
â”‚   â”œâ”€â”€ raw/                  # Original fMRI data
â”‚   â”‚   â”œâ”€â”€ AD/               # Alzheimer's patients
â”‚   â”‚   â””â”€â”€ CN/               # Healthy controls
â”‚   â”œâ”€â”€ aal3/                 # AAL3 brain atlas
â”‚   â”œâ”€â”€ metadata/             # Subject metadata
â”‚   â””â”€â”€ slices/               # 2D slice images
â”œâ”€â”€ model/                    # Trained neural network weights
â”‚   â”œâ”€â”€ capsnet/              # CapsNet-RNN weights
â”‚   â””â”€â”€ macadnnet/            # MCADNNet weights
â”œâ”€â”€ output/                   # Analysis results
â”‚   â”œâ”€â”€ activations/          # Neural activation maps
â”‚   â”œâ”€â”€ brain_maps/           # Brain region analysis results
â”‚   â””â”€â”€ visualizations/       # Generated plots and heatmaps
â”œâ”€â”€ graphql/                  # Neo4j knowledge graph
â”‚   â”œâ”€â”€ semantic_graph.graphml
â”‚   â”œâ”€â”€ nodes.csv
â”‚   â”œâ”€â”€ edges.csv
â”‚   â””â”€â”€ semantic_graph.png
â”œâ”€â”€ scripts/                  # Data processing and training scripts
â”œâ”€â”€ tests/                    # Testing and validation
â”œâ”€â”€ tools/                    # Utility scripts
â””â”€â”€ app.py                   # Streamlit web interface
```

---

## ğŸš€ Quick Start Guide

### 1. Data Setup

**Download fMRI Dataset:**

[â¡ï¸ Download Raw fMRI Dataset](https://u.pcloud.link/publink/show?code=kZEgL15ZhlezDWqfUEY3MkFwUK9Gtui7w0T7)

```bash
# Extract to data/raw/ directory
unzip data.zip -d data/
# Structure: data/raw/AD/ and data/raw/CN/
```

**Download Pre-trained Models:**

[â¡ï¸ Download Model Weights](https://u.pcloud.link/publink/show?code=kZ7gL15ZoCYrxwMqwwQmmBYDWfDmuy2GB4Ly)

```bash
# Place model weights
mkdir -p model/capsnet/
# Copy best_capsnet_rnn.pth to model/capsnet/
```

### 2. Knowledge Graph Setup

```bash
# Build Neo4j graph database
python -m tools.build_neo4j

# Verify Neo4j connection
python -c "from app.services.neo4j_connector import Neo4jConnector; client = Neo4jConnector(); print('Neo4j connected successfully!')"

# Test LLM providers
python -c "from app.services.llm_providers import llm_response; print(llm_response('Hello world', llm_provider='gemini'))"
python -c "from app.services.llm_providers import llm_response; print(llm_response('Hello world', llm_provider='aws_bedrock'))"
python -c "from app.services.llm_providers import llm_response; print(llm_response('Hello world', llm_provider='gpt-oss-20b', model='llama3.2'))"
```

### 3. Launch Web Interface

**Method 1: Web Interface (Recommended)**

```bash
# Start Streamlit application
streamlit run app.py

# Access at http://localhost:8501
```

**Method 2: Command Line**

```bash
# Run complete LangGraph pipeline
python -m app.graph.workflow

# Or use custom Python script
python -c "
from app.graph.workflow import app
result = app.invoke({
    'subject_id': 'sub-01',
    'fmri_scan_path': 'data/raw/CN/sub-01/scan.nii.gz',
    'model_path': 'model/capsnet/best_capsnet_rnn.pth',
    'error_log': [],
    'trace_log': []
})
print('Analysis completed:', result.get('classification_result'))
"
```

---

## ğŸ”¬ Development Commands

### Model Training

```bash
# Train CapsNet-RNN model (primary model)
python -m scripts.capsnet.train

# Train MCADNNet model (alternative)
python -m scripts.macadnnet.train

# Prepare training data
python -m scripts.preprocess.data_prepare
```

### Single Model Inference

```bash
# CapsNet-RNN inference
python -m scripts.capsnet.infer

# MCADNNet inference with activation extraction
python -m scripts.macadnnet.inference \
    --model model/macadnnet/best_model.pth \
    --input data/raw/AD/sub-14/dswausub-098_S_6601_task-rest_bold.nii.gz
```

### Group-Level Analysis Pipeline

```bash
# Complete workflow (run in order)
# 1. Generate activations for all subjects
python -m scripts.group.infer

# 2. Convert activation tensors to NIfTI
python -m scripts.group.act_nii

# 3. Resample to standard atlas space
python -m scripts.group.resample

# 4. Generate brain region statistics
python -m scripts.group.brain_map

# 5. Optional: Group average activations
python -m scripts.group.get_avg_act

# 6. Optional: Verify generated maps
python -m scripts.group.check_map
```

### Testing & Validation

```bash
# Test activation extraction
python -m tests.check_act

# Verify brain region mappings
python -m tests.brain_region

# Check model information
python -m tests.model_info

# Test individual components
python -m tests.image_explain
python -m tests.vertex  # Google ADK integration

# Test complete pipeline
python -m tests.test_complete_pipeline
```

---

## ğŸ–¥ï¸ Web Interface Usage

### Interface Overview

The Streamlit web interface provides an intuitive way to interact with Cognivex:

#### Sidebar Controls
- **Subject Selector**: Choose from available fMRI data subjects
- **Model Selector**: Select analysis model (CapsNet / MCADNNet)
- **Analysis Control**: Start analysis and emergency stop functionality
- **Model Information**: Display selected model details

#### Main Display Area
- **Progress Tracking**: Real-time analysis progress and status updates
- **Results Display**: Analysis results presentation area
- **Interactive Viewer**: Expandable 3D brain image viewer
- **Report Tabs**: Side-by-side Chinese and English clinical reports

### Usage Flow

1. **Launch Application**:
   ```bash
   streamlit run app.py
   # Access at http://localhost:8501
   ```

2. **Select Analysis Parameters**:
   - Choose subject from dropdown (format: `sub-01`, `sub-02`, etc.)
   - Select inference model (CapsNet recommended)
   - Review model information displayed

3. **Start Analysis**:
   - Click "Start Analysis" button
   - System locks all controls during analysis
   - Progress updates through stages:
     - Preparing analysis... (10%)
     - Loading data files... (20%)
     - Starting brain analysis workflow... (30%)
     - Running AI analysis pipeline... (50%)
     - Completing results... (90%)
     - Analysis successfully completed! (100%)

4. **Review Results**:
   - **Brain Activation Maps**: High-resolution brain activation heatmaps
   - **Prediction Validation**: True label vs model prediction comparison
   - **Interactive fMRI Viewer**: 4D fMRI data with time slider
   - **Bilingual Clinical Reports**: Comprehensive analysis in Chinese and English

---

## ğŸ› ï¸ Technology Stack

### ğŸ¤– AI/ML Framework
- **Agent Platform**: LangGraph 0.4.10 for workflow orchestration
- **LLM Architecture**: Modular provider system with unified interface
  - **Google Vertex AI Gemini**: Primary provider for multimodal analysis
  - **AWS Bedrock Claude**: Enterprise-grade text generation
  - **Ollama**: Local inference for privacy-sensitive environments
- **Deep Learning**: PyTorch 2.8.0, torchvision, torchinfo 1.8.0
- **Explainability**: grad-cam 1.5.5, custom activation analysis

### ğŸ§  Neuroimaging
- **Data Processing**: 
  - nibabel 5.3.2 (NIfTI file handling)
  - nilearn 0.11.1 (neuroimaging analysis)
  - scikit-image 0.25.2 (image processing)
- **Visualization**: 
  - matplotlib 3.10.6 (plotting)
  - seaborn 0.13.2 (statistical visualization)
  - plotly 6.3.0+ (interactive plots)
- **Brain Atlas**: AAL3 brain parcellation system

### ğŸ•¸ï¸ Knowledge Management
- **Graph Database**: Neo4j 5.28.2 with Python driver
- **Graph Processing**: NetworkX 3.5 for analysis
- **Query Engine**: Custom GraphRAG implementation
- **Data Formats**: GraphML, CSV exports

### ğŸ–¥ï¸ User Interface & Services
- **Web App**: Streamlit 1.49.1+ for interactive analysis
- **Backend**: Custom async runner with LangGraph workflows
- **API Capabilities**: FastAPI integration ready
- **Development**: Poetry package management with poethepoet tasks

---

## ğŸš¨ Troubleshooting

### Common Issues & Solutions

#### 1. CUDA/GPU Issues

**Problem**: CUDA unavailable or GPU memory insufficient
```bash
RuntimeError: CUDA out of memory
torch.cuda.is_available() returns False
```

**Solutions**:
```bash
# Check CUDA installation
nvidia-smi
python -c "import torch; print(torch.cuda.is_available())"

# Reinstall PyTorch with CUDA support (recommended)
poetry run poe autoinstall-torch-cuda
# OR manually:
python -m pip install light-the-torch
python -m light_the_torch install --upgrade torch torchaudio torchvision

# Clear GPU cache
python -c "import torch; torch.cuda.empty_cache()"
```

#### 2. Neo4j Connection Issues

**Problem**: Neo4j connection failures
```bash
ServiceUnavailable: Failed to establish connection to Neo4j database
```

**Solutions**:
```bash
# Check Neo4j service status
sudo systemctl status neo4j
docker ps | grep neo4j

# Restart Neo4j service
sudo systemctl restart neo4j
# OR for Docker:
docker restart neo4j-fmri

# Test connection
telnet localhost 7687
python -c "from app.services.neo4j_connector import Neo4jConnector; Neo4jConnector().test_connection()"

# Verify .env configuration
grep NEO4J .env
```

#### 3. LLM Provider Issues

**Problem**: LLM provider authentication or connection failures
```bash
ValueError: ä¸æ”¯æ´çš„ LLM ä¾›æ‡‰å•†: invalid_provider
google.auth.exceptions.DefaultCredentialsError: Could not automatically determine credentials
boto3.exceptions.NoCredentialsError: Unable to locate credentials
```

**Solutions**:
```bash
# Test each provider individually

# Gemini (Vertex AI)
export GOOGLE_APPLICATION_CREDENTIALS="./gcp-service-account.json"
export GOOGLE_CLOUD_PROJECT="your-project-id"
python -c "from app.services.llm_providers.gemini import handle_chat; print(handle_chat('test'))"

# Bedrock (AWS)
export AWS_ACCESS_KEY_ID="your_key"
export AWS_SECRET_ACCESS_KEY="your_secret"
python -c "from app.services.llm_providers.bedrock import handle_text; print(handle_text('test'))"

# Ollama (Local)
ollama serve &
ollama pull llama3.2
python -c "from app.services.llm_providers.ollama import handle_text; print(handle_text('test', model='llama3.2'))"

# Check unified interface
python -c "from app.services.llm_providers import llm_response; print(llm_response('test', llm_provider='gemini'))"
```

#### 4. Memory Issues

**Problem**: Insufficient system memory
```bash
MemoryError: Unable to allocate array
RuntimeError: out of memory
```

**Solutions**:
```bash
# Monitor memory usage
free -h
htop

# Clear Python cache
pip cache purge
python -c "import gc; gc.collect()"

# Optimize memory settings
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
```

#### 5. File Path & Permission Issues

**Problem**: File not found or permission denied
```bash
FileNotFoundError: No such file or directory
PermissionError: Permission denied
```

**Solutions**:
```bash
# Check file structure
ls -la data/raw/
ls -la model/capsnet/

# Fix permissions
chmod -R 755 data/ model/ output/
chown -R $USER:$USER data/ model/ output/

# Create missing directories
mkdir -p data/raw/{AD,CN} model/{capsnet,macadnnet} output/{activations,brain_maps}
```

#### 6. Streamlit Web App Issues

**Problem**: Web application fails to start or loads slowly
```bash
streamlit run app.py
ValueError: Session state is corrupted
```

**Solutions**:
```bash
# Clear Streamlit cache
streamlit cache clear
rm -rf ~/.streamlit/

# Check port availability
lsof -i :8501
# Kill conflicting processes if needed
kill -9 <PID>

# Use alternative port
streamlit run app.py --server.port 8502
```

### System Health Check Script

```python
#!/usr/bin/env python3
# health_check.py
import os, sys, torch, requests
from pathlib import Path

def health_check():
    print("ğŸ” Cognivex System Health Check")
    print("=" * 40)
    
    # Python version
    version = sys.version_info
    print(f"{'âœ…' if version >= (3, 11) else 'âŒ'} Python {version.major}.{version.minor}.{version.micro}")
    
    # CUDA availability
    cuda_available = torch.cuda.is_available()
    print(f"{'âœ…' if cuda_available else 'âš ï¸'} CUDA: {cuda_available}")
    
    # Environment file
    env_exists = Path('.env').exists()
    print(f"{'âœ…' if env_exists else 'âŒ'} .env file: {env_exists}")
    
    # Neo4j connection
    try:
        response = requests.get('http://localhost:7474', timeout=5)
        print(f"âœ… Neo4j web interface: {response.status_code}")
    except:
        print("âŒ Neo4j not accessible")
    
    # LLM Provider connectivity
    print("\nğŸ¤– LLM Provider Status:")
    
    # Test Gemini
    try:
        from app.services.llm_providers.gemini import handle_chat
        handle_chat("test")
        print("  âœ… Gemini (Vertex AI): Connected")
    except Exception as e:
        print(f"  âŒ Gemini: {str(e)[:50]}...")
    
    # Test Bedrock
    try:
        from app.services.llm_providers.bedrock import handle_text
        handle_text("test")
        print("  âœ… Bedrock (AWS): Connected")
    except Exception as e:
        print(f"  âš ï¸ Bedrock: {str(e)[:50]}...")
    
    # Test Ollama
    try:
        import ollama
        ollama.generate(model='llama3.2', prompt='test')
        print("  âœ… Ollama: Connected")
    except Exception as e:
        print(f"  âš ï¸ Ollama: {str(e)[:50]}...")
    
    # Directory structure
    print("\nğŸ“ Directory Structure:")
    required_dirs = ['data/raw/AD', 'data/raw/CN', 'model/capsnet', 'output', 'app/services/llm_providers']
    for dir_path in required_dirs:
        exists = Path(dir_path).exists()
        print(f"  {'âœ…' if exists else 'âŒ'} {dir_path}: {exists}")

if __name__ == "__main__":
    health_check()
```

Run health check:
```bash
python health_check.py
```

---

## ğŸ“ˆ Performance Optimization

### Hardware Recommendations

**Optimal Configuration**:
- **CPU**: Intel i7/i9 or AMD Ryzen 7/9
- **GPU**: NVIDIA RTX 3080/4080 or better (12GB+ VRAM)
- **RAM**: 32GB+ (minimum 16GB)
- **Storage**: NVMe SSD for data and models

**Cloud Deployment**:
- **AWS**: `p3.2xlarge` or `g4dn.xlarge`
- **Google Cloud**: `n1-highmem-4` + T4 GPU
- **Azure**: `Standard_NC6s_v3`

### Performance Tuning

```bash
# Neo4j memory optimization
# Edit /etc/neo4j/neo4j.conf:
# server.memory.heap.max_size=4G
# server.memory.pagecache.size=2G

# PyTorch optimization
export OMP_NUM_THREADS=4
export MKL_NUM_THREADS=4

# CUDA memory management
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512,roundup_power2_divisions:16
```

---

## ğŸ¤ Contributing

Cognivex is designed to be model-agnostic and extensible. Key areas for contribution:

### Development Setup

```bash
# Clone and setup development environment
git clone <repository-url>
cd semantic-KG
git checkout develop

# Install development dependencies
poetry install --with dev
poetry run pre-commit install

# Code quality tools
poetry run black .  # Format code
poetry run flake8 . # Linting
poetry run pytest --cov=app tests/  # Run tests
```

### Extension Areas

- **Model Integration**: Add support for new neural architectures
- **Knowledge Graph**: Expand brain region-disease relationships
- **Agent Capabilities**: Enhance multi-agent coordination
- **Visualization**: Improve interactive brain visualization
- **Multi-language Support**: Add support for additional languages

---

## ğŸ“œ License

See `license.txt` for details.

---

## ğŸ“ Support

For detailed usage instructions, see `instruction.md` (available in Chinese).

For technical issues:
1. Check this README's troubleshooting section
2. Run the system health check script
3. Review logs in the `output/` directory
4. Consult the comprehensive `instruction.md` guide

---

**Cognivex** - Making neuroimaging AI explainable and trustworthy for clinical applications.
